redis:
  host: database 
  port: 6379
queue:
  type: simple
  host: 127.0.0.1
  port: 8098
capacities:
  cpu: 2
  core: 8
  gpu: 8
  ram: 32
  disk: 100
  bw: 0
  burst_size: 0
  unit: 0
  mtu: 0
topology:
  children: null
  level: 0
  group: 0
  group_size: 0
  group_count: 0
  parent: null
  peer_agents:
    - 1
    - 2
  type: mesh
grpc:
  port: 20000 
  host: localhost
dtns:
  - name: dtn1
    ip: "192.168.1.10"
    user: "dtn_user"
    connectivity_score: 0.95
  - name: dtn2
    ip: "192.168.1.11"
    user: "dtn_user2"
    connectivity_score: 0.87
runtime:
  empty_timeout_seconds: 300
  reselection_timeout_s: 120
  peer_expiry_seconds: 45
  results_dir: swarm-multi
  shutdown_mode: manual
  total_agents: 5
  jobs_per_proposal: 20
  executor_workers: 10
  failure_threshold_seconds: 60        # Time before declaring agent as failed
  max_failed_agents: 0                 # Maximum tolerable failures (0 = unlimited)
  job_reassignment_enabled: true       # Enable automatic job reassignment
  aggressive_failure_detection: true  # Use proactive failure detection from gRPC
logging:
  ## The directory in which actor should create log files.
  ## This directory will be automatically created if it does not exist.
  log-directory: swarm-multi

  ## The filename to be used for actor's log file.
  log-file: agent

  ## The default log level for actor.
  log-level: INFO 

  ## actor rotates log files. You may specify how many archived log files to keep here.
  log-retain: 5

  ## actor rotates log files after they exceed a certain size.
  ## You may specify the file size that results in a log file being rotated here.
  log-size: 5000000

  logger: agent
job_selection:
  cost_weights:
    cpu: 0.4       # Relative importance of CPU utilization in job cost (0–1, sum of all weights should ≈ 1.0)
    ram: 0.3       # Relative importance of RAM utilization in job cost (0–1)
    disk: 0.2      # Relative importance of Disk utilization in job cost (0–1)
    gpu: 0.1       # Relative importance of GPU utilization in job cost (0–1)

  long_job_threshold: 20.0            # Execution time (in seconds) beyond which jobs incur extra penalty
  connectivity_penalty_factor: 1.0    # Multiplier for DTN connectivity penalty (0=no effect, >1 increases penalty severity)
  selection_threshold_pct: 10.0       # % above min cost allowed in candidate selection (lower = stricter, higher = more agents considered)
llm:
  enabled: true
  provider: openai            # or "none" to disable external calls
  model: "gpt-4o-mini"       # any text model that returns JSON
  temperature: 0.1
  timeout_seconds: 6
  use_for_selection: true     # use LLM to break ties across candidates
  prompts:
    cost: |-
      You are a distributed scheduler in a mesh topology with peer agents.
      Score this job for THIS agent (higher = better fit, 0-100).

      Key factors to consider:
      1. Resource headroom: Prefer agents with spare capacity
      2. Current load: Avoid overloaded agents (check peer context)
      3. DTN connectivity: Match job's data_in/data_out to agent's DTNs
      4. Load balancing: Favor underutilized agents in the mesh
      5. Execution time: Consider wall_time vs agent's current queue depth

      When PEER CONTEXT is provided, use it to make load-aware decisions.
      Compare this agent's load against peer loads to promote balanced distribution.

      Return JSON: {"score": <0-100>, "explanation": "<concise reason>"}
      Explanation should be max 1 sentence focusing on the key decision factor.
