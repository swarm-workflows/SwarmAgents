{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7992a2c7-0015-4c4e-8d92-be2af17eb4d2",
   "metadata": {},
   "source": [
    "# SWARM: Job Selection via Consensus\n",
    "\n",
    "In this notebook, we configure a topology that includes three Swarm Agent nodes and two Redis database nodes.\n",
    "\n",
    "### Agent Nodes\n",
    "These nodes host the Swarm Agents. Depending on the configuration, multiple agents can be launched on a single node. All agents collectively engage in a consensus process for job selection.\n",
    "\n",
    "### Database Nodes\n",
    "Redis instances run on these nodes and are responsible for maintaining the neighbor map, which is periodically updated by the agents.\n",
    "\n",
    "### Job Pool\n",
    "Jobs are generated as JSON files using the `python task_generator.py` script. When multiple agent nodes are used, the generated job file is distributed to all nodes to ensure a consistent job pool across all agents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa5c177-130e-4c5a-9fb6-9d4acf1eacf3",
   "metadata": {},
   "source": [
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8774091-068a-479e-9d01-77d6c3fef1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipaddress import ip_address, IPv4Address, IPv6Address, IPv4Network, IPv6Network\n",
    "import ipaddress\n",
    "\n",
    "from fabrictestbed_extensions.fablib.fablib import FablibManager as fablib_manager\n",
    "\n",
    "fablib = fablib_manager()\n",
    "                     \n",
    "fablib.show_config();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e004bc09-d316-49d2-8dec-6f5a0476eaaf",
   "metadata": {},
   "source": [
    "## Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d48abf-4528-48ab-b54a-713fd152b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_name = 'MySlice-swarm-1'\n",
    "swarm_node_name_prefix = \"agent\"\n",
    "swarm_node_count = 3\n",
    "\n",
    "database_node_name = \"database\"\n",
    "\n",
    "# Node profile parameters\n",
    "cores = 8\n",
    "ram = 32\n",
    "disk = 100\n",
    "image = \"default_ubuntu_22\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qph53h5eyg",
   "source": "## Configuration Parameters\n\n**Simplified Setup for Quick Testing:**\n\n- **`slice_name`**: Unique identifier for this FABRIC slice\n- **`swarm_node_count`**: 3 agent nodes (minimal setup for testing)\n- **`database_node_name`**: Name of Redis database host\n- **`cores/ram/disk`**: Higher resources (32GB RAM) for running multiple agents per node\n- **`image`**: Ubuntu 22 base image\n\n**Use Case:** This configuration is ideal for:\n- Quick functionality testing\n- Debugging agent behavior  \n- Development and prototyping\n- Learning the SWARM+ system\n\n**Contrast with Multi-Site Notebook:**\n- Multi-site: 110 nodes across 10 sites → Production-scale WAN evaluation\n- Simple: 3-4 nodes at few sites → Development and testing",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "fde0393b-b6fd-4fad-a1cc-8d928015766a",
   "metadata": {},
   "source": [
    "## Determine sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd77dbef-d362-469b-a17a-e3821262bc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sites = fablib.get_random_sites(count=swarm_node_count + 1, avoid=[\"NEWY\", \"CIEN\"])\n",
    "sites = [\"MAX\", \"FIU\", \"UCSD\", \"SALT\"]\n",
    "print(f'Preparing to create slice \"{slice_name}\" in site {sites}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e1363b-14a4-40bb-9236-98b65e7c8d7b",
   "metadata": {},
   "source": [
    "## Slice Creation\n",
    "\n",
    "- **Database Node**\n",
    "  - Allocate a node to host the Redis database. Ensure this node is connected to the L3 FabNetV4 network to enable communication with the agent nodes.\n",
    "\n",
    "- **Agent Cluster**\n",
    "  - Provision the number of nodes specified by `swarm_node_count` for deploying Swarm agents, ideally distributing them across multiple sites.\n",
    "  - Each agent node should also be connected to the L3 FabNetV4 network to facilitate inter-node communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45574763-be68-4c98-aa34-8a4790026f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Slice\n",
    "slice = fablib.new_slice(name=slice_name)\n",
    "\n",
    "database = slice.add_node(name=\"database\", site=sites[0], image=image, disk=disk, cores=cores, ram=ram)\n",
    "database.add_fabnet()\n",
    "database.add_post_boot_execute('sudo ssh-keygen -t rsa -N \"\" -f /root/.ssh/id_rsa')\n",
    "database.add_post_boot_execute(\"sudo git clone https://github.com/swarm-workflows/SwarmAgents.git /root/SwarmAgents\")\n",
    "database.add_post_boot_execute('sudo bash -c \"cd /root/SwarmAgents && ./install_ubuntu.sh\"')\n",
    "database.add_post_boot_execute(\"sudo /root/SwarmAgents/install_docker_ubuntu.sh\") \n",
    "database.add_post_boot_execute('sudo bash -c \"cd /root/SwarmAgents && docker compose up -d redis\"')\n",
    "\n",
    "# Add nodes for Agents and connect them to the kafka cluster\n",
    "for idx in range(swarm_node_count):\n",
    "    agent = slice.add_node(name=f\"{swarm_node_name_prefix}-{idx+1}\", site=sites[idx + 1], image=image, disk=disk, cores=cores, ram=ram)\n",
    "    agent.add_fabnet()\n",
    "    agent.add_post_boot_execute('sudo ssh-keygen -t rsa -N \"\" -f /root/.ssh/id_rsa')\n",
    "    agent.add_post_boot_execute(\"sudo git clone -b agent-topology --single-branch https://github.com/swarm-workflows/SwarmAgents.git /root/SwarmAgents\")\n",
    "    agent.add_post_boot_execute(\"sudo /root/SwarmAgents/install_ubuntu.sh\") \n",
    "    agent.add_post_boot_execute(\"sudo /root/SwarmAgents/install_docker_ubuntu.sh\") \n",
    "\n",
    "# Submit Slice Request\n",
    "slice.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nxjhdje9lub",
   "source": "### Automated Post-Boot Configuration\n\nEach node is configured automatically via `add_post_boot_execute()` commands:\n\n**Database Node Setup:**\n1. Generate SSH keys for root access\n2. Clone SwarmAgents repository from GitHub\n3. Run Ubuntu installation script (`install_ubuntu.sh`)\n4. Install Docker\n5. Launch Redis container via Docker Compose\n\n**Agent Node Setup:**\n1. Generate SSH keys for root access\n2. Clone SwarmAgents repository (specific branch: `agent-topology`)\n3. Run Ubuntu installation script  \n4. Install Docker\n\n**Advantages of Post-Boot Automation:**\n- Nodes ready to use immediately after provisioning\n- Consistent environment across all nodes\n- No manual SSH configuration required\n- Repeatable deployments\n\n**Wait Time:** Expect ~5-10 minutes for all post-boot scripts to complete after slice becomes active",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4f0db8-0fec-4642-afeb-f9c831f40d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice = fablib.get_slice(slice_name)\n",
    "slice.list_nodes();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0057db21-60f3-484f-bc0a-277ae554d38d",
   "metadata": {},
   "source": [
    "## Configure Hostnames\n",
    "\n",
    "On each agent node, add an entry to the `/etc/hosts` file mapping the database node’s IP address to its hostname. This ensures agents can resolve and connect to the database node correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ab5c26-507d-409d-aaff-57b8fcfbaa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = slice.get_node(database_node_name)\n",
    "database_addr = database.get_interface(network_name=f\"FABNET_IPv4_{database.get_site()}\").get_ip_addr()\n",
    "\n",
    "for n in slice.get_nodes():\n",
    "    if n.get_name() == database_node_name:\n",
    "        continue\n",
    "    n.execute(f'sudo sh -c \\'echo \"{database_addr} database\" >> /etc/hosts\\'')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54c3ff0-c0f4-4eb1-bb8e-cf9c427ff8c6",
   "metadata": {},
   "source": [
    "## Running SWARM-MULTI Consensus Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830bdb7c-3f84-4a03-93d2-9f84894fc3ff",
   "metadata": {},
   "source": [
    "### Simplified Topology Overview - Single Node\n",
    "\n",
    "- The **database** is launched on a dedicated `database` node.\n",
    "- **All agents** are deployed on a single node (e.g., `agent-1`).\n",
    "\n",
    "- A job pool is generated using a JSON file named `tasks.json`.\n",
    "\n",
    "- The system supports two modes of agent communication:\n",
    "  - **Ring**: Agents are grouped into rings of 5; one agent from each ring connects to form higher-level rings until a single ring remains.\n",
    "  - **Mesh**: Every agent communicates with all other agents.\n",
    "\n",
    "- The communication mode is configurable via the `topology` field in `config_swarm_multi.yml`:\n",
    "  - For **Mesh**, set `topology.peer_agents` to `\"all\"`.\n",
    "  - For **Ring**, set `topology.peer_agents` to a comma-separated list of peer agent IDs.\n",
    "\n",
    "- Use the provided script to launch the agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xr2dijuyhyo",
   "source": "## Manual Experiment Workflow\n\nUnlike the multi-site notebook which uses automated batch testing, this notebook demonstrates **manual agent launch** for learning and debugging.\n\n**Workflow Overview:**\n\n1. **Install Dependencies** → Run pip install on agent nodes\n2. **Generate Job Pool** → Create tasks.json with synthetic jobs\n3. **Launch Agents** → SSH into node and run launch script\n4. **Monitor Progress** → Check job completion via Redis queries\n5. **Stop Agents** → Graceful shutdown\n6. **Collect Results** → Download logs and plots from `swarm-multi/` directory\n\n**Why Manual Mode?**\n- Better visibility into agent startup process\n- Easier debugging of configuration issues\n- Educational: understand each step of system operation\n- Fine-grained control over agent launch timing\n\n**Topology Options:**\nThe system supports two communication modes (configured in `config_swarm_multi.yml`):\n\n- **Mesh**: Every agent connects to all others (`topology.peer_agents = \"all\"`)\n  - Best for: Small-scale testing (≤30 agents)\n  - Characteristics: O(n²) connections, fast consensus, high message overhead\n  \n- **Ring**: Agents organized in hierarchical rings of 5\n  - Best for: Larger deployments (30+ agents)\n  - Characteristics: O(√n) message routing, lower overhead, slightly higher latency",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8692e713-6d41-47ec-b9bf-d9f9dd04f2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = slice.get_node(\"agent-1\")\n",
    "\n",
    "stdout, stderr = agent1.execute(f'sudo bash -c \"cd /root/SwarmAgents && pip3.11 install -r requirements.txt\"', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f98403-e042-4def-a1b8-f79de94eff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_count = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7572c9a-385f-4fde-9a86-44d9b194a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = slice.get_node(\"agent-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448f550d-dae0-4751-8eb8-9e5c1df47e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout, stderr = agent1.execute(f'sudo bash -c \"cd /root/SwarmAgents && python3.11 task_generator.py {task_count}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180318fc-e7f7-4962-bda1-8f247cbdffaa",
   "metadata": {},
   "source": [
    "### GRPC Hack\n",
    "NOTE: There is some issue with gRPC version mismatch, the pip install commands are a hack to resolve it and should be executed only once\n",
    "\n",
    "```bash\n",
    "sudo su -\n",
    "cd SwarmAgents\n",
    "pip3.11 install protobuf==3.20.3\n",
    "pip3.11 install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Launching the Agents\n",
    "\n",
    "- SSH into `agent1` using the command mentioned earlier, then run the following commands to start the agents:\n",
    "\n",
    "NOTE: There is some issue with gRPC version mismatch, the pip install commands are a hack to resolve it and should be executed only once\n",
    "\n",
    "```bash\n",
    "sudo su -\n",
    "cd SwarmAgents\n",
    "./swarm-multi-start.sh 10 ring database\n",
    "```\n",
    "\n",
    "### Verifying Completed Jobs\n",
    "\n",
    "- To check the number of completed jobs, run:\n",
    "```bash\n",
    "python3.11 dump_tasks.py --host database --key job --count\n",
    "```\n",
    "\n",
    "- If the output matches the total number of expected jobs (e.g., 100), you’re ready to stop the agents.\n",
    "\n",
    "### Stopping the Agents\n",
    "\n",
    "- Use the following command to gracefully stop all running agents:\n",
    "```bash\n",
    "./swarm-multi-kill.sh\n",
    "```\n",
    "\n",
    "After shutdown, all logs and generated plots will be available in the `SwarmAgents/swarm-multi` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea761bb8-ec59-46f8-b9f8-fa7c0ff1dc0c",
   "metadata": {},
   "source": [
    "### Multi Node Topology\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0511bb9c-754a-44b3-b000-b9539c62d4f1",
   "metadata": {},
   "source": [
    "### Delete the Slice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psm4g52frmi",
   "source": "## Collecting Results\n\nAfter running the manual experiment, collect your data from the agent nodes.\n\n### Results Location\nAll logs, metrics, and plots are stored in:\n```\n/root/SwarmAgents/swarm-multi/\n```\n\n### Download Results via SSH\n```bash\n# From your local machine\nscp -r ubuntu@<agent-1-ip>:/root/SwarmAgents/swarm-multi ./local-results\n```\n\n### What You'll Find\n```\nswarm-multi/\n├── agent-<id>.log              # Per-agent execution logs\n├── agent-<id>.csv              # Job assignments per agent  \n├── all_jobs.csv                # Consolidated job data\n├── metrics.json                # Performance statistics\n├── topology.png                # Network topology diagram (if generated)\n└── latency_cdf.png             # Latency distribution plot (if generated)\n```\n\n### Quick Analysis Commands\n\n**Check job completion:**\n```bash\npython3.11 dump_tasks.py --host database --key job --count\n```\n\n**Verify all agents completed:**\n```bash\ncat swarm-multi/metrics.json | jq '.completed_jobs, .total_jobs'\n```\n\n**View latency stats:**\n```bash\ncat swarm-multi/metrics.json | jq '.avg_latency, .p95_latency, .p99_latency'\n```\n\n### Troubleshooting\n- **No output files?** Check agent logs for errors: `tail -100 swarm-multi/agent-*.log`\n- **Jobs not completing?** Verify Redis connectivity: `redis-cli -h database ping`\n- **Consensus issues?** Check network connectivity between agents",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0654c171-7238-4433-8a6b-94455c90df34",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da9a33f-df81-48e1-9526-a0e9b4bc39a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}